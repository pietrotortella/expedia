{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import csv\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#some numbers that takes a while to compute, and I dont want to run the computation again\n",
    "nrows_train = 37670294\n",
    "nrows_test = 2528244\n",
    "nuser_ids = 1198787\n",
    "nbookings_train = 3000694\n",
    "\n",
    "base_path = os.getcwd()\n",
    "data_path = os.path.join(base_path, 'data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1 milion lines of train require 192MB of memory\n",
    "\n",
    "def read_csv_in_gzip(filename, beginning=0, end=1000000):\n",
    "    \"\"\"Reads a csv file in gzip, returns a pandas DataFrame\"\"\"\n",
    "    nrows = end - beginning\n",
    "    with gzip.open(filename) as fzip:\n",
    "        df = pd.read_csv(fzip, skiprows=beginning, nrows=nrows)\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def update_userid_keys(id_list, df):\n",
    "    \"\"\"Adds to the id_list the user_id keys in the dataframe df\n",
    "    \"\"\"\n",
    "    new_ids = list(set(id_list + list(df['user_id'].values)))\n",
    "    \n",
    "    return new_ids\n",
    "\n",
    "\n",
    "def find_all_id(filename):\n",
    "    \"\"\"Returns a list with all the user_id in the gzip filename\"\"\"\n",
    "    chunk_size = 10 ** 5\n",
    "    id_list = []\n",
    "    chunk_counter = 0\n",
    "    with gzip.open(filename) as fzip:\n",
    "        for chunk in pd.read_csv(fzip, chunksize=chunk_size):\n",
    "            if chunk_counter % 50 == 0:    \n",
    "                print('Processing chunk n. {} (chunk size equal to {})'.format(chunk_counter, chunk_size))\n",
    "            chunk_counter += 1\n",
    "            id_list = update_userid_keys(id_list, chunk)\n",
    "    \n",
    "    return id_list\n",
    "\n",
    "\n",
    "def write_list_to_txt(lista, filename, append=True):\n",
    "    \"\"\"If append is true and the file already exists, it appends the result at the bottom \n",
    "    of the file\"\"\"\n",
    "    with open(filename, 'a') as w_file:\n",
    "        for item in lista:\n",
    "            w_file.write(\"%s\\n\" % item)\n",
    "    pass\n",
    "\n",
    "\n",
    "def count_lines(filename, is_zip=False, print_exponent=20):\n",
    "    \"\"\"\n",
    "    returns the number of lines in the gzip file.\n",
    "    \"\"\"\n",
    "    if is_zip:\n",
    "        with gzip.open(filename) as fzip:\n",
    "            for i, l in enumerate(fzip):\n",
    "                if print_exponent is not None:\n",
    "                    if i % (2 ** print_exponent) == 0:\n",
    "                        clear_output(wait=True)\n",
    "                        print('got to line', i)\n",
    "                        sys.stdout.flush()\n",
    "    else:\n",
    "        with open(filename) as f:\n",
    "            for i, _ in enumerate(f):\n",
    "                if print_exponent is not None:\n",
    "                    if i % (2 ** print_exponent) == 0:\n",
    "                        clear_output(wait=True)\n",
    "                        print('got to line', i)\n",
    "                        sys.stdout.flush()\n",
    "    return i + 1\n",
    "\n",
    "\n",
    "def count_files(directory_path):\n",
    "    return len(os.listdir(directory_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_magnitude(number):\n",
    "    magnitude = math.log(number, 10)\n",
    "    n_unit = int(magnitude / 3)\n",
    "    if n_unit == 0:\n",
    "        unit = ''\n",
    "        etodivide = 0\n",
    "    elif n_unit == 1:\n",
    "        unit = 'K'\n",
    "        etodivide = 3\n",
    "    elif n_unit == 2:\n",
    "        unit = 'M'\n",
    "        etodivide = 6\n",
    "    elif n_unit == 3:\n",
    "        unit = 'G'\n",
    "        etodivide = 9\n",
    "    else:\n",
    "        unit = 'E' + str(magnitude)\n",
    "        etodivide = magnitude\n",
    "        \n",
    "    return unit, etodivide\n",
    "\n",
    "\n",
    "def print_processing_status(current_line, tot_line, s_time, frequency=None, pre_message=None):\n",
    "    if frequency is None:\n",
    "        frequency = int(tot_line / 100)\n",
    "    message = 'Processing line {0:.1f}{4} (of {1:.1f}{4}). \\t Elapsed time: {2:.0f} secs, \\t ETA: {3:.0f} secs.' \n",
    "    if current_line % frequency == 0 and current_line > 1:\n",
    "        unit, etodivide = get_magnitude(tot_line)\n",
    "        loc_time = time.time() - s_time\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        if pre_message is not None:\n",
    "            print(pre_message)\n",
    "        print(message.format(current_line / (10 ** etodivide), tot_line / (10 ** etodivide), \n",
    "              loc_time, (tot_line / current_line - 1) * loc_time, unit))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        \n",
    "def convert_time(seconds):\n",
    "    secs = int(seconds) % 60\n",
    "    mins = (seconds - secs)/60 % 60\n",
    "    hours = ((seconds - secs)/60 - mins)/60 \n",
    "    if hours != 0:\n",
    "        return str(hours) + 'h' + str(mins) + 'm'\n",
    "    else:\n",
    "        return str(mins) + 'm' + str(secs) + 's'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_in_csv(filename, df, columns=[], replace=False):\n",
    "    \"\"\"\n",
    "    Writes a dataframe to a csv file.\n",
    "    Columns are the name of the columns to write to file.\n",
    "    If replace, it overwrites the file if it exists, \n",
    "    otherwise it appends to the end of it.\n",
    "    The variable already_exists is to write the columns in the first line,\n",
    "    when initializing a new file.\n",
    "    \"\"\"\n",
    "    if replace:\n",
    "        write_mode = 'w'\n",
    "        already_exists = False\n",
    "    else:\n",
    "        write_mode = 'a'\n",
    "        already_exists = False\n",
    "        try:\n",
    "            with open(filename, 'r') as f:\n",
    "                already_exists = True\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "    \n",
    "    nlines = len(df)\n",
    "    l_chunk = int(nlines / 20)\n",
    "          \n",
    "    start_time = time.time()\n",
    "    with open(filename, write_mode) as f:\n",
    "        scrivi = csv.writer(f, delimiter=',')\n",
    "        if not already_exists:\n",
    "            scrivi.writerow(columns)\n",
    "        for count, i in enumerate(df.index):\n",
    "            scrivi.writerow(df.ix[i].values)\n",
    "            \n",
    "            print_processing_status(count, nlines, start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_csv_for_userid(df):\n",
    "    \"\"\"\n",
    "    Splits a dataframe according to the user_id key.\n",
    "    Returns a dictionary, with keys the user_id.\n",
    "    \"\"\"\n",
    "    local_ids = list(set(df['user_id']))\n",
    "    \n",
    "    splitted = dict()\n",
    "    for j in local_ids:\n",
    "        splitted[j] = pd.DataFrame(columns=df.columns)\n",
    "        splitted[j] = df.ix[df['user_id'] == j]\n",
    "        \n",
    "    return splitted\n",
    "\n",
    "\n",
    "def create_users_file(bigzipfile, basename='user_', location=os.path.join(os.getcwd(), 'data/'), n_tot_lines=0):\n",
    "    \"\"\"\n",
    "    Process the bigzipfile, and creates a csv file for each user_id key, with the corresponding lines.\n",
    "    \"\"\"\n",
    "    chunk_size = 10 ** 5\n",
    "    n_chunks = int(n_tot_lines/chunk_size) + 1\n",
    "    chunk_counter = 0\n",
    "    print('Beginning to read:')\n",
    "    with gzip.open(bigzipfile) as fzip:\n",
    "        for chunk in pd.read_csv(fzip, chunksize=chunk_size):\n",
    "            chunk_counter += 1\n",
    "            print('Processing chunk n. {} (of {})'.format(chunk_counter, n_chunks))\n",
    "            local_splitted = split_csv_for_userid(chunk)\n",
    "            for i in local_splitted.keys():\n",
    "                filename = basename + str(i) + '.csv'\n",
    "                filename = location + filename\n",
    "                write_in_csv(filename, local_splitted[i], local_splitted[i].columns)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_users(nusers, nsamples):\n",
    "    \"\"\"\n",
    "    Selects a sample of user_ids.\n",
    "    \"\"\"\n",
    "    all_ids = list(range(nusers-1))\n",
    "    random.shuffle(all_ids)\n",
    "    \n",
    "    uids = []\n",
    "    for i in range(nsamples):\n",
    "        uids += [all_ids.pop()]\n",
    "    \n",
    "    all_filenames = np.array(os.listdir(udata_path))\n",
    "    sample_ids = np.zeros(nsamples, dtype=int)\n",
    "    for i in range(nsamples):\n",
    "        sample_ids[i] = int(all_filenames[uids[i]][5:-4])\n",
    "        \n",
    "    return sample_ids\n",
    "\n",
    "\n",
    "def write_sample_train(sample_ids, train_filename='train.csv.gz', out_filename='data/train_sample.csv'):\n",
    "    \"\"\"\n",
    "    A function to select a sample of user_ids and write them to file\n",
    "    \"\"\"\n",
    "    \n",
    "    with gzip.open(train_filename, 'rt', newline='') as infile:\n",
    "        with open(out_filename, 'w', newline='') as outfile:\n",
    "            legge = csv.reader(infile)\n",
    "            scrive = csv.writer(outfile, delimiter=',')\n",
    "            \n",
    "            unit, etodivide = get_magnitude(nrows_train)\n",
    "            status_string = 'Processing line {0:.1f}{4} (of {1:.1f}{4}). \\t'\n",
    "            statur_string += ' Elapsed time: {2:.0f} secs, \\t ETA: {3:.0f} secs.' \n",
    "            start_time = time.time()\n",
    "            \n",
    "            p_wrote = False\n",
    "            previous_id = -1\n",
    "            for i, row in enumerate(legge):             \n",
    "                if i>2 : \n",
    "                    print_processing_status(i, nrows_train, start_time, frequency=50000, pre_message='Writing...')\n",
    "                \n",
    "                if i == 0:\n",
    "                    print('Initializing...')\n",
    "                    scrive.writerow(row)\n",
    "                else:\n",
    "                    if int(row[7]) == previous_id:\n",
    "                        if p_wrote:\n",
    "                            scrive.writerow(row)\n",
    "                        else:\n",
    "                            pass                    \n",
    "                    else:\n",
    "                        if int(row[7]) in sample_ids:\n",
    "                            scrive.writerow(row)\n",
    "                            p_wrote = True\n",
    "                        else: \n",
    "                            p_wrote = False\n",
    "                    previous_id = int(row[7])\n",
    "                        \n",
    "            print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_df_onfile(amount, amount2=0, infilename='data/train_booking_lines.csv', printfile=False, \n",
    "                    printcomplement=False, filename1='data/cv_data.csv', filename2='data/train_data.csv'):\n",
    "    \"\"\"\n",
    "    Selects from a dataframe stored in the csv file infilename a subset big as amount.\n",
    "    If printfile, it prints it to the csv file filename1, otherwise it returns the indices \n",
    "    and the dataframe.\n",
    "    If printcomplement, it prints to file also the complement of the selected dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Loading file...')\n",
    "    df_original = pd.read_csv(infilename)\n",
    "    nlines0 = len(df_original)\n",
    "    print('Done!')\n",
    "    \n",
    "    print('Indices selection...')\n",
    "    indici_s = list(range(nlines0))\n",
    "    random.shuffle(indici_s)\n",
    "    \n",
    "    indici1 = []\n",
    "    indici2 = []\n",
    "    \n",
    "    for i in range(nlines0):\n",
    "        if i < amount:\n",
    "            indici1 += [indici_s.pop()]\n",
    "        elif i < amount + amount2:\n",
    "            indici2 += [indici_s.pop()]\n",
    "    \n",
    "    indici1 = sorted(indici1)\n",
    "    if printcomplement:\n",
    "        indici2 = sorted(indici2)\n",
    "    print('Done!')\n",
    "    \n",
    "    print('Dataframe splittin...')\n",
    "    df_split1 = df_original.ix[indici1]\n",
    "    if printcomplement:\n",
    "        df_split2 = df_original.ix[indici2]\n",
    "    print('Done!')\n",
    "    \n",
    "    if printfile:\n",
    "        print('Writing file 1...')\n",
    "        write_in_csv(filename=filename1, df=df_split1, replace=True, columns=df_split1.columns, print_status=True)\n",
    "        print('Done!')\n",
    "        \n",
    "        if printcomplement:\n",
    "            print('Writing file 2...')\n",
    "            write_in_csv(filename=filename2, df=df_split2, replace=True, \n",
    "                         columns=df_split2.columns, print_status=True)\n",
    "            print('Done!')\n",
    "    else:\n",
    "        return indici, cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_cluster_dict_to_file(cdict, filename):\n",
    "    \"\"\"\n",
    "    cdict is a ditionary of dictionaries, whose first key is the first label you want to\n",
    "    have an idea of, while the key of the second-layer dict are the hotel cluster;\n",
    "    the value represents how many hotels of that cluster appear with the first label.\n",
    "    This writes such a dict to file in the following way:\n",
    "    for each key1, it prints its value and the length of its keys in a row, separated by a comma;\n",
    "    then in the following rows are the key2 and the value, separated by a comma.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(filename, 'w') as ofile:\n",
    "        for k1 in cdict.keys():\n",
    "            length1 = len(cdict[k1])\n",
    "            key1_line = k1 + ',' + str(length1) + '\\n'\n",
    "            ofile.write(key1_line)\n",
    "            for k2 in cdict[k1].keys():\n",
    "                line = k2 + ',' + str(cdict[k1][k2]) + '\\n'\n",
    "                ofile.write(line)\n",
    "    \n",
    "\n",
    "def read_cluster_dict_from_file(filename):\n",
    "    toret = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    with open(filename) as ifile:\n",
    "        legge = csv.reader(ifile)\n",
    "        while True:\n",
    "            try:\n",
    "                line1 = next(legge)\n",
    "                k1 = line1[0]\n",
    "                l1 = line1[1]\n",
    "                for i in range(int(l1)):\n",
    "                    line2 = next(legge)\n",
    "                    k2 = line2[0]\n",
    "                    v2 = line2[1]\n",
    "                    toret[k1][k2] = int(v2)\n",
    "    \n",
    "            except StopIteration:\n",
    "                break\n",
    "    \n",
    "    return toret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dest_features(n_feat=3):\n",
    "    \"\"\"\n",
    "    using PCA, it finds some features from the file destinations.csv.gz\n",
    "    \"\"\"\n",
    "    destinations = read_csv_in_gzip('destinations.csv.gz')\n",
    "    pca = PCA(n_components=n_feat)\n",
    "    dest_small = pca.fit_transform(destinations[[\"d{0}\".format(i + 1) for i in range(149)]])\n",
    "    dest_small = pd.DataFrame(dest_small)\n",
    "    dest_small[\"srch_destination_id\"] = destinations[\"srch_destination_id\"]\n",
    "    \n",
    "    return dest_small    \n",
    "\n",
    "\n",
    "def augment_df(df):\n",
    "    \"\"\"\n",
    "    augments the dataframe df with additional features\n",
    "    \"\"\"\n",
    "    df[\"date_time\"] = pd.to_datetime(df[\"date_time\"])\n",
    "    df[\"srch_ci\"] = pd.to_datetime(df[\"srch_ci\"], format='%Y-%m-%d', errors=\"coerce\")\n",
    "    df[\"srch_co\"] = pd.to_datetime(df[\"srch_co\"], format='%Y-%m-%d', errors=\"coerce\")\n",
    "    \n",
    "    props = {}\n",
    "    for prop in [\"month\", \"day\", \"hour\", \"minute\", \"dayofweek\", \"quarter\"]:\n",
    "        props[prop] = getattr(df[\"date_time\"].dt, prop)\n",
    "    \n",
    "    carryover = [p for p in df.columns if p not in [\"date_time\", \"srch_ci\", \"srch_co\"]]\n",
    "    for prop in carryover:\n",
    "        props[prop] = df[prop]\n",
    "    \n",
    "    date_props = [\"month\", \"day\", \"dayofweek\", \"quarter\"]\n",
    "    for prop in date_props:\n",
    "        props[\"ci_{0}\".format(prop)] = getattr(df[\"srch_ci\"].dt, prop)\n",
    "        props[\"co_{0}\".format(prop)] = getattr(df[\"srch_co\"].dt, prop)\n",
    "    props[\"stay_span\"] = (df[\"srch_co\"] - df[\"srch_ci\"]).astype('timedelta64[h]')\n",
    "        \n",
    "    ret = pd.DataFrame(props)\n",
    "    \n",
    "    ret = ret.join(dest_small, on=\"srch_destination_id\", how='left', rsuffix=\"dest\")\n",
    "    ret = ret.drop(\"srch_destination_iddest\", axis=1)\n",
    "    return ret\n",
    "\n",
    "dest_small = create_dest_features()\n",
    "\n",
    "\n",
    "def create_augmented_train_file(in_filename='train.csv.gz',\n",
    "                                out_filename='data/train_augmented.csv',\n",
    "                                tot_lines=nrows_train\n",
    "                               ):\n",
    "    \"\"\"\n",
    "    it creates a file with the augmented features\n",
    "    \"\"\"    \n",
    "    \n",
    "    chunk_size = 2 ** 10\n",
    "    n_chunks = int(tot_lines / chunk_size) + 1\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with gzip.open(in_filename) as infile:\n",
    "        reader = pd.read_csv(infile, chunksize=chunk_size)\n",
    "        for i, chunk in enumerate(reader):\n",
    "            print_processing_status(i, n_chunks, start_time, frequency=1)\n",
    "            augmented = augment_df(chunk) \n",
    "            write_in_csv(filename=out_filename, df=augmented, columns=augmented.columns)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_augmented_train_file(in_filename='train.csv.gz',\n",
    "                           out_filename= os.path.join(data_path, 'test_augmented.csv'),\n",
    "                           tot_lines=nrows_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing line 2.5K (of 2.5K). \t Elapsed time: 1012 secs, \t ETA: 0 secs.\n"
     ]
    }
   ],
   "source": [
    "create_augmented_train_file(in_filename='test.csv.gz',\n",
    "                           out_filename= os.path.join(data_path, 'test_augmented.csv'),\n",
    "                           tot_lines=nrows_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
